#!/usr/bin/env python3
import re
import os

os.environ["RCUTILS_CONSOLE_OUTPUT_FORMAT"] = "{message}"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from operator import itemgetter
# from deep_translator import GoogleTranslator

from dotenv import load_dotenv

load_dotenv()

prompt_template_str = """
ë‹¹ì‹ ì€ Turtlebot Robotì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ëª…ë ¹ì— ëŒ€í•´ ë‹¤ìŒ 3ê°€ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. 'Action','Time', 'Conversation' 

ë‹¹ì‹ ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Actionì€ [ë©ˆì¶¤, ì „ì§„, í›„ì§„, ì¢ŒíšŒì „, ìš°íšŒì „, ì¢Œë¡œëŒì•„, ìš°ë¡œëŒì•„] ì…ë‹ˆë‹¤.
'ë©ˆì¶¤'ì€ ë¡œë´‡ì´ ì•„ë¬´ ë™ì‘ë„ í•˜ì§€ ì•ŠëŠ” ê²ƒì…ë‹ˆë‹¤. Actionì˜ default ê°’ì€ 'ë©ˆì¶¤'ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ ëŒ€í™”ë¥¼ í•  ê²½ìš° 'ë©ˆì¶¤'ì…ë‹ˆë‹¤.
'ì „ì§„'ì€ ë¡œë´‡ì´ ì•ìœ¼ë¡œ ê°€ëŠ” ë™ì‘ì…ë‹ˆë‹¤. 
'í›„ì§„'ì€ ë¡œë´‡ì´ ë’¤ë¡œ ê°€ëŠ” ë™ì‘ì…ë‹ˆë‹¤. 
'ì¢ŒíšŒì „'ì€ ë¡œë´‡ì´ ì œìë¦¬ íšŒì „ì„ ì¢Œì¸¡ ë°©í–¥ìœ¼ë¡œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
'ìš°íšŒì „'ì€ ë¡œë´‡ì´ ì œìë¦¬ íšŒì „ì„ ìš°ì¸¡ ë°©í–¥ìœ¼ë¡œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
'ì¢Œë¡œëŒì•„'ëŠ” ë¡œë´‡ì´ ì¢Œì¸¡ ë°©í–¥ìœ¼ë¡œ ì› íšŒì „ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¢Œì¸¡ìœ¼ë¡œ ì›ì„ ê·¸ë¦¬ë©° ì£¼ë³€ì„ ë•ë‹ˆë‹¤. 
'ìš°ë¡œëŒì•„'ëŠ” ë¡œë´‡ì´ ìš°ì¸¡ ë°©í–¥ìœ¼ë¡œ ì› íšŒì „ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ì¸¡ìœ¼ë¡œ ì›ì„ ê·¸ë¦¬ë©° ì£¼ë³€ì„ ë•ë‹ˆë‹¤.

Timeì˜ default ê°’ì€ 2 ì…ë‹ˆë‹¤. 
Timeì€ Actionì„ ìˆ˜í–‰í•˜ëŠ” ì‹œê°„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  

Conversationì€ ë‹¹ì‹ ì´ í•˜ê³  ì‹¶ì€ ë§ì„ í•˜ë©´ë©ë‹ˆë‹¤. í˜„ì¬ ì…ë ¥ë°›ì€ ëª…ë ¹ê³¼, í–‰ë™, ìƒí™©ì— ì í•©í•œ ëŒ€ë‹µì„ í•˜ì„¸ìš”.

ë‹¤ìŒì€ ì¶œë ¥ ì˜ˆì‹œ ì…ë‹ˆë‹¤. 

[Question1]
ì•ìœ¼ë¡œ ê°€ 

[Answer1]
Action: ì „ì§„
Time: 2
Conversation: ì „ì§„í•˜ê² ìŠµë‹ˆë‹¤.

[Question2]
ë” ë§ì´ ì•ìœ¼ë¡œ ê°€ 

[Answer2]
Action: ì „ì§„
Time: 4
Conversation: ë” ì•ìœ¼ë¡œ ì „ì§„í•˜ê² ìŠµë‹ˆë‹¤.


ìœ„ ì˜ˆì œì™€ ê°™ì€ ëŒ€ë‹µ ìƒì„± í˜•ì‹ì— ë§ê²Œ ì•„ë˜ [Question]ì— ëŒ€ë‹µí•˜ì„¸ìš”. ì§€ê¸ˆê¹Œì§€ ëŒ€í™” ë‚´ì—­ì„ ì°¸ê³ í•˜ì„¸ìš”. 

[Previous Chat Histroy]
{chat_history}

[Question]
{question}

[Answer]
"""


def get_llm(streaming: bool = False, llm_type: str ='openai'):
    
    if llm_type == 'openai':
        return ChatOpenAI(
            streaming=streaming,
            temperature=0,
            model_name="gpt-3.5-turbo",
        )
    elif llm_type == 'ollama':
        return ChatOllama(
            streaming=streaming,
            temperature=0,
            model_name="llama3.2",
        )


class LLMNode(Node):
    def __init__(self):
        super().__init__('llm_node')
        self.llm_move_publisher = self.create_publisher(String, '/move_command', 10) 
        self.llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        self.prompt = PromptTemplate.from_template(prompt_template_str)
        self.chat_store = {}
        # self.get_session_history = get_session_history()
        self.chain = self.init_chain()
        self.get_logger().info(f"\nChain complete!\n")
    
    def get_session_history(self, session_id):

        if session_id not in self.chat_store:
            self.chat_store[session_id] = ChatMessageHistory()
        history = self.chat_store[session_id]
        if len(history.messages) > 10:
            history.messages = history.messages[-10:]
        return history
    
    def init_chain(self):
        chain = ( 
                    {
                        "question": itemgetter("question"),         # RunnablePassthrough(),
                        "chat_history": itemgetter("chat_history"),
                    } 
                    | self.prompt 
                    | self.llm 
                    | StrOutputParser() 
                )
        
        rag_with_history = RunnableWithMessageHistory(
            chain,
            self.get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
            input_messages_key="question",  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key
            history_messages_key="chat_history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
        )
        
        return rag_with_history
    

    def process_query(self, query):
        try:
            response = self.chain.invoke({"question": query}, config={"configurable": {"session_id": "rag123"}}) 
            response_text = response.content if hasattr(response, "content") else str(response)
            return response_text
        except Exception as e:
            self.get_logger().error(f"error occurred: {e}")
            return None
        
    def llm_run(self):
        while rclpy.ok():
            try:                
                query = input("ğŸ’¬ Human: \n")
                if query.lower() in ['quit', 'exit']:
                    self.get_logger().info("ë…¸ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # query = GoogleTranslator(source='auto', target='ko').translate(query) 
                response = self.process_query(query)

                if response:
                    msg = String()
                    msg.data = response
                    con_match = re.search(r"Conversation:\s*(.+)", response, re.DOTALL)
                    con = con_match.group(1).strip() if con_match else ""
            
                    self.llm_move_publisher.publish(msg)
                    # con = GoogleTranslator(source='auto', target='vi').translate(con) 
                    self.get_logger().info(f"\nğŸ¤– Robot: \n{con} \n")
                else:
                    self.get_logger().info(f"\nğŸ¤– Robot: \n{response} \n")
                
            except KeyboardInterrupt:
                self.get_logger().info("ë…¸ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                self.get_logger().error(f"error occurred: {e}")

def main(args=None):
    rclpy.init(args=args)
    node = LLMNode()

    try:
        node.llm_run()
    except Exception as e:
        node.get_logger().error(f"ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
    
    
# ros2 run turtle_llm llm_node